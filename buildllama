#!/usr/bin/env python3

import os
import subprocess
import sys
import shutil

def run_cmd(cmd, check=True, shell=True):
    """Run a command and return the result"""
    print(f"Running: \033[35m{cmd}\033[0m")
    result = subprocess.run(cmd, shell=shell, capture_output=True, text=True)
    if check and result.returncode != 0:
        print(f"Command failed: {cmd}")
        print(f"stdout: {result.stdout}")
        print(f"stderr: {result.stderr}")
        sys.exit(1)
    return result

def print_status(msg):
    """Print status message in yellow"""
    print(f"\033[33m{msg}\033[0m")

def check_nvidia_smi():
    """Check if nvidia-smi is available"""
    try:
        result = subprocess.run(["nvidia-smi", "--query-gpu=compute_cap", "--format=csv"],
                              capture_output=True, text=True)
        return result.returncode == 0
    except FileNotFoundError:
        return False

def get_cuda_arch():
    """Get CUDA architecture from nvidia-smi"""
    result = run_cmd('nvidia-smi --query-gpu=compute_cap --format=csv | tac | head -n1 | tr -d "."')
    return result.stdout.strip()

def check_cmake():
    """Check if cmake is installed, install if needed"""
    try:
        subprocess.run(["cmake", "--version"], capture_output=True, check=True)
        print_status("cmake is already installed")
    except (FileNotFoundError, subprocess.CalledProcessError):
        print("cmake not found, attempting to install...")
        if shutil.which("apt"):
            run_cmd("sudo apt update && sudo apt install -y cmake")
        else:
            print("apt not found, please install cmake manually")
            sys.exit(1)

def get_cpu_count():
    """Get number of CPU cores"""
    result = run_cmd('cat /proc/cpuinfo | grep processor | wc -l')
    return result.stdout.strip()

def main():
    # Clone llama.cpp repo
    print_status("Cloning llama.cpp repository...")
    os.chdir("/tmp")
    if os.path.exists("llama.cpp"):
        shutil.rmtree("llama.cpp")
    run_cmd("git clone --depth 1 https://github.com/ggerganov/llama.cpp.git")
    os.chdir("llama.cpp")

    # Check and install cmake
    check_cmake()

    # Get CPU count for parallel build
    cpu_count = get_cpu_count()
    print_status(f"Using {cpu_count} cores for build")

    # Check for NVIDIA GPU
    has_nvidia = check_nvidia_smi()

    # Build GPU version if NVIDIA GPU is available
    if has_nvidia:
        print()
        print_status("NVIDIA GPU detected, building with CUDA support...")
        cuda_arch = get_cuda_arch()
        print_status(f"CUDA architecture: {cuda_arch}")

        run_cmd(f'cmake -B build -DCMAKE_CUDA_ARCHITECTURES="{cuda_arch}"')
        run_cmd(f'cmake --build build --config Release -j {cpu_count}')

        # Test GPU build
        os.chdir("build/bin")
        run_cmd("./llama-server --help")
        print_status("GPU build test successful")
        os.chdir("../..")

    # Build CPU version
    print()
    cpu_build_dir = "cpubuild" if has_nvidia else "build"
    if has_nvidia:
        print_status("Building CPU version...")
    else:
        print_status("No NVIDIA GPU detected, building CPU-only version...")

    run_cmd(f"cmake -B {cpu_build_dir}")
    run_cmd(f"cmake --build {cpu_build_dir} --config Release -j {cpu_count}")

    # Test CPU build
    os.chdir(f"{cpu_build_dir}/bin")
    run_cmd("./llama-server --help")
    print_status("CPU build test successful")
    os.chdir("../..")

    print()
    print_status("Build completed successfully!")
    if has_nvidia:
        print(f"GPU binaries available in: \033[35m{os.getcwd()}/build/bin/\033[0m")
        print(f"CPU binaries available in: \033[35m{os.getcwd()}/cpubuild/bin/\033[0m")
    else:
        print(f"CPU binaries available in: \033[35m{os.getcwd()}/build/bin/\033[0m")

if __name__ == "__main__":
    main()
