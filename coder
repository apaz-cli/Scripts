#!/bin/sh


MODEL_NAME="Qwen3-Coder-30B-A3B-Instruct-Q5_K_S.gguf"
MODEL_REPO="unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"
CHAT_TEMPL="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct/raw/main/chat_template.jinja"

PROXIED_MODEL_NAME="openai/qwen3-coder"

MODELS_PATH="$HOME/.ggml"
MODEL_PATH="$MODELS_PATH/$MODEL_NAME"
TEMPL_PATH="$MODELS_PATH/$MODEL_NAME""_chat_template.jinja"
MODEL_URL="https://huggingface.co/$MODEL_REPO/resolve/main/$MODEL_NAME?download=true"

LLAMA_CPP_PORT="9876"
PROXIED_PORT="9877"

REPO_PATH="/tmp/llama.cpp"
LITELLM_PATH="/tmp/cc_litellm_venv"
REPO_URL="https://github.com/ggml-org/llama.cpp"

ANY_BUILD=0

# Download model only if it doesn't exist
if [ ! -f "$MODEL_PATH" ]; then
  ANY_BUILD=1
  echo "Downloading $MODEL_REPO."
  mkdir -p "$(dirname "$MODEL_PATH")"
  curl -sSL "$MODEL_URL" > "$MODEL_PATH" &
  curl -sSL "$CHAT_TEMPL" > "$TEMPL_PATH" &
fi

# Create a venv with litellm proxy
if [ ! -f "$LITELLM_PATH" ]; then
  ANY_BUILD=1
  echo "Setting up litellm proxy."
  mkdir -p "$(dirname "$LITELLM_PATH")"
  python -m venv "$LITELLM_PATH"
  "$LITELLM_PATH/bin/pip" install 'litellm[proxy]' >/dev/null 2>&1 &
fi

# Clone llama.cpp only if it doesn't exist
if [ ! -d "$REPO_PATH" ]; then
  ANY_BUILD=1
  echo "Compiling llama.cpp."
  git clone "$REPO_URL" "$REPO_PATH" >/dev/null 2>&1

  WD="$(pwd)"
  cd "$REPO_PATH"

  echo "Building llama.cpp."
  cmake -B build -DGGML_CUDA=ON >/dev/null 2>&1
  cmake --build build --config Release -j "$(nproc)" >/dev/null 2>&1 &

  cd "$WD"
fi

if [ "$ANY_BUILD" = 1 ]; then
  echo "Waiting for llama.cpp build and model download to complete."
  wait
fi

# Start Model
CUDA_VISIBLE_DEVICES=0 \
"$REPO_PATH/build/bin/llama-server" \
  -m "$MODEL_PATH" \
  --host 0.0.0.0 \
  --port "$LLAMA_CPP_PORT" \
  --chat-template-file "$TEMPL_PATH" \
  --jinja \
  -c 32768 \
  -fa 1 \
  --temp 0.7 \
  --min-p 0 \
  --top-p 0.80 \
  --top-k 20 \
  --repeat-penalty 1.05 \
  >/tmp/llama_server_logs 2>&1 \
  &

# Create Litellm config
cat > /tmp/litellm_config.yaml <<EOF
model_list:
  - model_name: $MODEL_NAME
    litellm_params:
      model: $PROXIED_MODEL_NAME
      api_base: http://127.0.0.1:$LLAMA_CPP_PORT/v1
      api_key: sk-1234-meow

general_settings:
  master_key: sk-1234-meow
EOF

# Start Litellm Proxy
"$LITELLM_PATH/bin/litellm" \
  --config /tmp/litellm_config.yaml \
  --port "$PROXIED_PORT" \
  --num_workers 2 \
  --telemetry False \
  --drop_params \
  >/tmp/litellm_logs 2>&1 \
  &

# Wait for services to start
sleep 2

# Start claude code
export ANTHROPIC_AUTH_TOKEN="sk-1234-meow"
export ANTHROPIC_BASE_URL="http://127.0.0.1:$PROXIED_PORT"
claude --model "$PROXIED_MODEL_NAME"
